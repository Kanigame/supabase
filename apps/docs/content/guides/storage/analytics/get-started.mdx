---
title: 'Getting Started'
subtitle: ''
---

<Admonition type="warning">
  This feature is in **Private Alpha**. API stability and backward compatibility are not guaranteed
  at this stage.
</Admonition>

Analytics Buckets use [Apache Iceberg](https://iceberg.apache.org/), an open-table format for managing large analytical datasets.
You can interact with them using tools such as [PyIceberg](https://py.iceberg.apache.org/), [Apache Spark](https://spark.apache.org/) or any client which supports the [standard Iceberg REST Catalog API](https://editor-next.swagger.io/?url=https://raw.githubusercontent.com/apache/iceberg/main/open-api/rest-catalog-open-api.yaml).

In this guide we will create our specialized **Analytics Bucket** and perform some basic operations

## Create your first Analytics Bucket

You can create an Analytics Bucket using either the Supabase SDK or the Supabase Dashboard.

### Using the Supabase SDK

```ts
import { createClient } from '@supabase/supabase-js'

const supabase = createClient('https://your-project.supabase.co', 'your-service-key')

supabase.storage.createBucket('my-analytics-bucket', {
  type: 'ANALYTICS',
})
```

### Using the Supabase Dashboard

1. Navigate to the Storage section in the Supabase Dashboard.
2. Click on "Create Bucket".
3. Enter a name for your bucket (e.g., my-analytics-bucket).
4. Select "Analytics Bucket" as the bucket type.

<img alt="Storage schema design" src="/docs/img/storage/iceberg-bucket.png" />

## Create S3 credentials

An Iceberg client will need to interact with the underlying S3 Bucket.

To authenticate your client, create S3 Credentials go to [**Project Settings > Storage**](https://supabase.com/dashboard/project/_/settings/storage), for more information, see the [S3 Authentication Guide](https://supabase.com/docs/guides/storage/s3/authentication). We will support other authentication methods in the future.

You will now have an **Access Key** and a **Secret Key** that you can use to authenticate your Iceberg client.

## Connecting to the Iceberg Catalog

To connect to the Iceberg Catalog, you will need an Iceberg client which supports the REST Catalog interface.

You can use [PyIceberg](https://py.iceberg.apache.org/) or [Apache Spark](https://spark.apache.org/) to interact with it.
The catalog url will look like this:

```
https://<your-supabase-project>.supabase.co/storage/v1/iceberg
```

To authenticate with the Iceberg REST Catalog, you need to provide a valid Supabase **Service key** as a Bearer token.

```
curl \
  --request GET -sL \
  --url 'https://<your-supabase-project>.supabase.co/storage/v1/iceberg/v1/config?warehouse=<bucket-name>' \
  --header 'Authorization: Bearer <your-service-key>'
```

## Using PyIceberg

Here's a comprehensive example using PyIceberg with clearly separated configuration:

```python
from pyiceberg.catalog import load_catalog
import pyarrow as pa
import datetime

# Supabase project ref
PROJECT_REF = "<your-supabase-instance>"

# Configuration for Iceberg REST Catalog
WAREHOUSE = "your-analytic-bucket-name"
TOKEN = "SERVICE_KEY"

# Configuration for S3-Compatible Storage
S3_ACCESS_KEY = "KEY"
S3_SECRET_KEY = "SECRET"
S3_REGION = "PROJECT_REGION"

S3_ENDPOINT = f"https://{PROJECT_REF}.supabase.co/storage/v1/s3"
CATALOG_URI = f"https://{PROJECT_REF}.supabase.co/storage/v1/iceberg"

# Load the Iceberg catalog
catalog = load_catalog(
    "analytics-bucket",
    type="rest",
    warehouse=WAREHOUSE,
    uri=CATALOG_URI,
    token=TOKEN,
    **{
        "py-io-impl": "pyiceberg.io.pyarrow.PyArrowFileIO",
        "s3.endpoint": S3_ENDPOINT,
        "s3.access-key-id": S3_ACCESS_KEY,
        "s3.secret-access-key": S3_SECRET_KEY,
        "s3.region": S3_REGION,
        "s3.force-virtual-addressing": False,
    },
)

# Create namespace if it doesn't exist
catalog.create_namespace_if_not_exists("default")

# Define schema for your Iceberg table
schema = pa.schema([
    pa.field("event_id", pa.int64()),
    pa.field("event_name", pa.string()),
    pa.field("event_timestamp", pa.timestamp("ms")),
])

# Create table (if it doesn't exist already)
table = catalog.create_table_if_not_exists(("default", "events"), schema=schema)

# Generate and insert sample data
current_time = datetime.datetime.now()
data = pa.table({
    "event_id": [1, 2, 3],
    "event_name": ["login", "logout", "purchase"],
    "event_timestamp": [current_time, current_time, current_time],
})

# Append data to the Iceberg table
table.append(data)

# Scan table and print data as pandas DataFrame
df = table.scan().to_pandas()
print(df)
```
