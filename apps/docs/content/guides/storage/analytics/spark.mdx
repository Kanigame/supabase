---
title: 'Using Apache Spark with Analytics Buckets'
---

<Admonition type="warning">
  This feature is in **Private Alpha**. API stability and backward compatibility are not guaranteed
  at this stage.
</Admonition>

Apache Spark allows distributed analytical queries against Iceberg Buckets.

**Example Spark Usage**

```python
from pyspark.sql import SparkSession

# Supabase project ref
PROJECT_REF = "<your-supabase-instance>"

# Configuration for Iceberg REST Catalog
WAREHOUSE = "your-analytic-bucket-name"
TOKEN = "SERVICE_KEY"

# Configuration for S3-Compatible Storage
S3_ACCESS_KEY = "KEY"
S3_SECRET_KEY = "SECRET"
S3_REGION = "PROJECT_REGION"

S3_ENDPOINT = f"https://{PROJECT_REF}.supabase.co/storage/v1/s3"
CATALOG_URI = f"https://{PROJECT_REF}.supabase.co/storage/v1/iceberg"

# Initialize Spark session with Iceberg configuration
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("SupabaseIceberg") \
    .config("spark.driver.host", "127.0.0.1") \
    .config("spark.driver.bindAddress", "127.0.0.1") \
    .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,org.apache.iceberg:iceberg-aws-bundle:1.6.1') \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.my_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.my_catalog.type", "rest") \
    .config("spark.sql.catalog.my_catalog.uri", CATALOG_URI) \
    .config("spark.sql.catalog.my_catalog.warehouse", WAREHOUSE) \
    .config("spark.sql.catalog.my_catalog.token", TOKEN) \
    .config("spark.sql.catalog.my_catalog.s3.endpoint", S3_ENDPOINT) \
    .config("spark.sql.catalog.my_catalog.s3.path-style-access", "true") \
    .config("spark.sql.catalog.my_catalog.s3.access-key-id", S3_ACCESS_KEY) \
    .config("spark.sql.catalog.my_catalog.s3.secret-access-key", S3_SECRET_KEY) \
    .config("spark.sql.catalog.my_catalog.s3.remote-signing-enabled", "false") \
    .config("spark.sql.defaultCatalog", "my_catalog") \
    .getOrCreate()

# SQL Operations
spark.sql("CREATE NAMESPACE IF NOT EXISTS analytics")

spark.sql("""
    CREATE TABLE IF NOT EXISTS analytics.users (
        user_id BIGINT,
        username STRING
    )
    USING iceberg
""")

spark.sql("""
    INSERT INTO analytics.users (user_id, username)
    VALUES (1, 'Alice'), (2, 'Bob'), (3, 'Charlie')
""")

result_df = spark.sql("SELECT * FROM analytics.users")
result_df.show()
```
